---
layout: page
title: "Call for Papers"
permalink: /call-for-papers/
---

<!-- # Call for Papers -->



### Overview and Scope: 


Large Language Models (LLMs) such as GPT-4, Claude, and Gemini have demonstrated remarkable capabilities across a wide range of natural language processing tasks. However, as these models are increasingly integrated into high-stakes applications—such as healthcare, law, education, and scientific research—their reliability and trustworthiness have become critical concerns.

Uncertainty quantification (UQ) is an essential component in building reliable AI systems. Understanding and representing what a model "does not know" is key to ensuring robust decision-making, improving model calibration, mitigating hallucinations, and enabling human-AI collaboration.

This workshop aims to bring together researchers and practitioners from academia and industry to discuss recent advances, challenges, and future directions related to uncertainty estimation and reliability in LLMs. We encourage interdisciplinary submissions from the machine learning, NLP, AI safety, and HCI communities.

### Topic of Interest: 


We welcome original contributions on topics including, but not limited to:

1. Techniques for uncertainty quantification in LLMs (e.g., Bayesian methods, ensembles, MC Dropout, conformal prediction)

1. Calibration and confidence estimation in LLM outputs

1. Detecting and mitigating hallucinations and overconfident failures

1. Benchmarking LLM reliability across tasks and domains

1. Epistemic vs aleatoric uncertainty in language models

1. Uncertainty-aware prompting and decoding strategies

1. Evaluating trust and interpretability in the presence of uncertainty

1. Human-AI collaboration and decision-making with uncertain LLMs

1. Robustness under distributional shifts and adversarial inputs

1. Applications of UQ in downstream tasks (e.g., summarization, question answering, code generation)

### Submission Details: 


We invite submissions of the following types:

1. **Full Papers** (4–8 pages, excluding references): Novel research contributions with empirical or theoretical insights.

1. **Extended Abstracts** (2 pages): Work-in-progress, preliminary results, or visionary ideas.

1. **Demo Proposals** (2 pages): Interactive tools or systems demonstrating uncertainty-aware LLM applications. Description of interactive tools or applications related to LLM reliability or uncertainty. 
    The submission must include a link to the codebase that implements the tool/application or, if hosted online, a link to the live system.

All submissions should be formatted using the AAAI 2026 author kit and will be peer-reviewed by the program committee. Accepted papers will be presented either as talks or posters. The workshop will be non-archival, and authors are free to submit their work to other venues.

### Important Dates:


TBD


For questions or inquiries, please contact us at: desi.ivanova@stats.ox.ac.uk, weiliumg@gmail.com, hvishwakarma@cs.wisc.edu .





