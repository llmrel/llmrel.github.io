---
layout: page
title: "Call for Papers"
permalink: /call-for-papers/
---

<!-- # Call for Papers -->



### Overview and Scope: 


Large Language Models (LLMs) such as GPT-4, Claude, and Gemini have demonstrated remarkable capabilities across a wide range of natural language processing tasks. However, as these models are increasingly integrated into high-stakes applications—such as healthcare, law, education, and scientific research—their reliability and trustworthiness have become critical concerns.

Uncertainty quantification (UQ) is an essential component in building reliable AI systems. Understanding and representing what a model "does not know" is key to ensuring robust decision-making, improving model calibration, mitigating hallucinations, and enabling human-AI collaboration.

This workshop aims to bring together researchers and practitioners from academia and industry to discuss recent advances, challenges, and future directions related to uncertainty estimation and reliability in LLMs. We encourage interdisciplinary submissions from the machine learning, NLP, AI safety, and HCI communities.

### Topic of Interest: 


We welcome original contributions from probabilistic machine learning, statistics, engineering, NLP, HCI, and related fields. Submissions may address (but are not limited to) the following topics:


    1. **Defining and measuring reliability in LLM contexts.** How should reliability be conceptualised for stochastic, learned systems? What metrics and evaluation frameworks are most appropriate for capturing different aspects of unreliable behaviour?

    1. **Identifying and quantifying sources of unreliability.** These include (but are not limited to) hallucinations, overconfidence, prompt sensitivity, poor calibration, and brittleness to domain or distribution shift. How can we systematically characterise and measure failure modes?

    1. **Detecting or predicting unreliable behaviour.** What methods can identify when an LLM is likely to fail or produce unreliable outputs? For example, methods for uncertainty estimation and calibration, failure detection and prediction, and so on.

    1. **Reliability focused training and alignment.** Can we design training objectives, fine-tuning methods, or alignment (post-training) techniques to encourage reliability? Is there a trade-off between capabilities and reliability?

    1. Practical deployment considerations.** How can reliability signals be integrated into decision-making systems? What are the human factors and UX considerations for communicating (un)reliability to end users?

    1. Benchmarking and evaluation.** How should reliability be measured and evaluated in practice? What are the limitations of current benchmarks, and how can we develop more robust evaluation frameworks?


### Submission Details: 


We invite submissions of the following types:

1. **Full Papers** (4–8 pages, excluding references): Novel research contributions with empirical or theoretical insights.

1. **Extended Abstracts** (2 pages): Work-in-progress, preliminary results, or visionary ideas.

1. **Demo Proposals** (2 pages): Interactive tools or systems demonstrating uncertainty-aware LLM applications. Description of interactive tools or applications related to LLM reliability or uncertainty. 
    The submission must include a link to the codebase that implements the tool/application or, if hosted online, a link to the live system.

Please submit your contributions via OpenReview. All submissions should be anonymous and formatted using the AAAI 2026 author kit. 
We welcome submissions of original, unpublished material, as well as work that is currently under review or published within the past six months.

**Accepted papers**.  The workshop is non-archival, however, the accepted papers will be made public via OpenReview.
All accepted papers will be presented as posters and the top contributions will also be presented as spoltlight presentations. 
The workshop will be non-archival, and authors are free to submit their work to other venues.

**Reviewing**. Authors should nominate at least one person to review submissions, with an expected reviewing load of at most 3 papers.

**LLM use policy**. In the preparation of manuscripts, the use of LLMs is only allowed as a general-purpose writing assist tool.

### Important Dates:


TBD


For questions or inquiries, please contact us at: desi.ivanova@stats.ox.ac.uk, weiliumg@gmail.com, hvishwakarma@cs.wisc.edu .





