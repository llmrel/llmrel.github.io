---
layout: page
title: "Overview"
permalink: /overview/
body_class: with-background
---

<!-- # Overview -->

Welcome to the AAAI 2026 Workshop! Large Language Models (LLMs) have demonstrated remarkable _capabilities_ across a range of natural‑language and multi-modal tasks, including understanding, generation, and reasoning.
However, raw capability alone is not sufficient for real-world deployment.
The key barrier to adoption, particularly in  high‑risk domains such as healthcare, education or law, is _reliability_.This workshop will bring together researchers and practitioners to explore existing and emerging **quantitative approaches for defining, evaluating, and improving LLM reliability**.
The goal is to sharpen our understanding of what reliability means for LLMs and to develop practical tools and benchmarks to enable confident deployment at scale.

#### Key issues and questions:

1. **Defining and measuring reliability in LLM contexts**. How should reliability be conceptualised for stochastic, learned systems? What metrics and evaluation frameworks are most appropriate for capturing different aspects of unreliable behaviour?
1. **Identifying and quantifying sources of unreliability**. These include (but are not limited to) hallucinations, overconfidence, prompt sensitivity, poor calibration, and brittleness to domain or distribution shift. How can we systematically characterise and measure failure modes?

1. **{Detecting or predicting unreliable behaviour**. What methods can identify when an LLM is likely to fail or produce unreliable outputs? For example, methods for uncertainty estimation and calibration, failure detection and prediction, and so on.
1. **Reliability focused training and alignment**. Can we design training objectives, fine-tuning methods, or alignment (post-training) techniques to encourage reliability? Is there a trade-off between capabilities and reliability?
1. **Practical deployment considerations**. How can reliability signals be integrated into decision-making systems? What are the human factors and UX considerations for communicating (un)reliability to end users?
1. **Benchmarking and evaluation**. How should reliability be measured and evaluated in practice? What are the limitations of current benchmarks, and how can we develop more robust evaluation frameworks?


For further information and should you have any inquiries, please contact: desi.ivanova@stats.ox.ac.uk, weiliumg@gmail.com, hvishwakarma@cs.wisc.edu.



